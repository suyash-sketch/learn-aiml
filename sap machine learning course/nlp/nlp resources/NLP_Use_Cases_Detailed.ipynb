{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701bb7de",
   "metadata": {},
   "source": [
    "# NLP Use Cases: Canonicalisation, Phonetic Hashing, Edit Distance, Spell Corrector, and Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2654f76",
   "metadata": {},
   "source": [
    "## 1. Canonicalisation\n",
    "\n",
    "* 1. Canonicalisation\n",
    "     \n",
    "Canonicalisation refers to the process of standardizing text representations by making them consistent and uniform. This process is vital for text pre-processing in NLP pipelines.\n",
    "### Standardizing text representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757181b9-bfa4-4d93-a7ba-691931fd3e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased Text: natural language processing is amazing!\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Lowercasing\n",
    "text = \"Natural Language Processing is Amazing!\"\n",
    "lowercased_text = text.lower()\n",
    "print(\"Lowercased Text:\", lowercased_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef6b4e4-92b1-4b8a-8e8a-6239990d5924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without Punctuation: Hello World NLP is fun\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Example 2: Removing Punctuation\n",
    "text = \"Hello, World! NLP is fun.\"\n",
    "text_no_punc = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Text without Punctuation:\", text_no_punc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b37cae-19aa-4108-8f24-60d7dc820013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Text: I am learning NLP and I cannot stop!\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"can't\": \"cannot\", \"won't\": \"will not\", \"I'm\": \"I am\"}\n",
    "text = \"I'm learning NLP and I can't stop!\"\n",
    "expanded = ' '.join([contractions.get(word, word) for word in text.split()])\n",
    "print(\"Expanded Text:\", expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc95b16-5968-48fe-a283-31188f4a1e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\A1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['running', 'jump', 'easily', 'fairly']\n",
      "Stemmed Words: ['run', 'jump', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Example 4: Lemmatization and Stemming\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "words = ['running', 'jumps', 'easily', 'fairly']\n",
    "\n",
    "# Lemmatization (Contextual)\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Stemming (Simple)\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized)\n",
    "print(\"Stemmed Words:\", stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4758097-300b-4dc4-9183-1676740063dc",
   "metadata": {},
   "source": [
    "# 2. Phonetic Hashing\n",
    "Phonetic hashing converts words into standardized phonetic representations, helping with fuzzy matching or misspelling corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39fd7f92-e743-4610-a989-5a6b4068296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting phonetics\n",
      "  Downloading phonetics-1.0.5.tar.gz (8.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: phonetics\n",
      "  Building wheel for phonetics (setup.py): started\n",
      "  Building wheel for phonetics (setup.py): finished with status 'done'\n",
      "  Created wheel for phonetics: filename=phonetics-1.0.5-py2.py3-none-any.whl size=8720 sha256=92df207883d5c21223d4b74521ec52cca1af6feda2ab7edbf02c9f91d0558831\n",
      "  Stored in directory: c:\\users\\a1\\appdata\\local\\pip\\cache\\wheels\\4e\\d0\\ca\\0abf0e0c628782f163861d5daf61c192f61b611aa235c40f52\n",
      "Successfully built phonetics\n",
      "Installing collected packages: phonetics\n",
      "Successfully installed phonetics-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install phonetics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aab4b86-4f83-4661-b0a6-3fd6b5ab185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soundex (Smith): S5030\n",
      "Soundex (Smyth): S5030\n"
     ]
    }
   ],
   "source": [
    "import phonetics\n",
    "\n",
    "# Example 1: Soundex\n",
    "print('Soundex (Smith):', phonetics.soundex('Smith'))\n",
    "print('Soundex (Smyth):', phonetics.soundex('Smyth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2bea62-2a28-4b42-9500-bed129003c5c",
   "metadata": {},
   "source": [
    "###  Metaphone\n",
    "Metaphone generates phonetic representations for English words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2401200c-f256-4556-8c62-d17a1d5d432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaphone (Knight): NT\n",
      "Metaphone (Night): NT\n"
     ]
    }
   ],
   "source": [
    "print('Metaphone (Knight):', phonetics.metaphone('Knight'))\n",
    "print('Metaphone (Night):', phonetics.metaphone('Night'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cc5eb10-409d-4a15-80c8-14a06ba2e570",
   "metadata": {},
   "source": [
    "Metaphone accounts for silent letters and irregular pronunciations.\n",
    "Helps in approximate name matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a9366-8784-4759-9031-134f7d524175",
   "metadata": {},
   "source": [
    "# Double Metaphone\n",
    "More accurate phonetic matching using Double Metaphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59baebc1-b8ca-4d24-8983-d2c89f57e514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Metaphone (Bier): BR\n",
      "Double Metaphone (Beer): BR\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "\n",
    "# Example 3: Double Metaphone\n",
    "print('Double Metaphone (Bier):', jellyfish.metaphone('Bier'))\n",
    "print('Double Metaphone (Beer):', jellyfish.metaphone('Beer'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7308bcd2-5f97-46bd-a4a3-7e455fe691a9",
   "metadata": {},
   "source": [
    "Double Metaphone handles non-English names effectively.\n",
    "Used in identity resolution systems."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0056b889-f584-4663-956c-0311c9fb159d",
   "metadata": {},
   "source": [
    "NYSIIS (New York State Identification and Intelligence System)\n",
    "NYSIIS is another phonetic algorithm used in databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4807850f-813d-48ec-b516-6de6e70fc96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYSIIS (Christopher): CRASTAFAR\n",
      "NYSIIS (Kristopher): CRASTAFAR\n"
     ]
    }
   ],
   "source": [
    "print('NYSIIS (Christopher):', jellyfish.nysiis('Christopher'))\n",
    "print('NYSIIS (Kristopher):', jellyfish.nysiis('Kristopher'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb7cd0-fc00-499b-8d11-366881c6f552",
   "metadata": {},
   "source": [
    "# 3. Edit Distance\n",
    "Measures similarity between two strings based on the minimum operations (insert, delete, replace) required."
   ]
  },
  {
   "cell_type": "raw",
   "id": "24388552-b20c-4495-b33b-f637dcd95808",
   "metadata": {},
   "source": [
    "Levenshtein Distance\n",
    "Classic Levenshtein Distance calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6294d57c-1c9b-4f6d-9da1-4d8faa2ff1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Levenshtein in c:\\users\\a1\\jupyter\\lib\\site-packages (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from Levenshtein) (3.12.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b418a95d-fbb0-42f6-ac73-0fb4351f57b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein (kitten, sitting): 3\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein as lev\n",
    "\n",
    "# Example 1: Levenshtein Distance\n",
    "print('Levenshtein (kitten, sitting):', lev.distance('kitten', 'sitting'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc14c2a5-6f45-47a7-ab74-91561e2f658f",
   "metadata": {},
   "source": [
    "Measures character-level similarity.\n",
    "Used in spell checking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d09e3-4ce1-403b-949e-87ae460b238a",
   "metadata": {},
   "source": [
    "# Damerau-Levenshtein Distance\n",
    "Accounts for transpositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbe3488b-024f-43fb-b382-e31a797b84e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jellyfish in c:\\users\\a1\\jupyter\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3aaa6b90-3cd2-46a7-99e9-334ae13ac73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damerau-Levenshtein (cat, cast): 1\n",
      "Damerau-Levenshtein (hello, hlelo): 1\n",
      "Damerau-Levenshtein (apple, applf): 1\n",
      "Damerau-Levenshtein (data, adta): 1\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "\n",
    "# Damerau-Levenshtein distance examples\n",
    "print('Damerau-Levenshtein (cat, cast):', jellyfish.damerau_levenshtein_distance('cat', 'cast'))\n",
    "print('Damerau-Levenshtein (hello, hlelo):', jellyfish.damerau_levenshtein_distance('hello', 'hlelo'))\n",
    "print('Damerau-Levenshtein (apple, applf):', jellyfish.damerau_levenshtein_distance('apple', 'applf'))\n",
    "print('Damerau-Levenshtein (data, adta):', jellyfish.damerau_levenshtein_distance('data', 'adta'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c506f9e-54fa-441e-8513-1f1e2edba50c",
   "metadata": {},
   "source": [
    "The jellyfish.damerau_levenshtein_distance() function calculates the edit distance between two strings, considering transpositions as one operation.\n",
    "This makes it more accurate for typo corrections and name matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38f1cf8-651b-4f2b-bb75-cecb213430d8",
   "metadata": {},
   "source": [
    "## Jaro-Winkler Distance\n",
    "Measures string similarity with more weight on common prefixes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa8eb3ea-3bf5-4b69-8ff2-a79854f03f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaro-Winkler (hello, helo): 0.9533333333333333\n"
     ]
    }
   ],
   "source": [
    "print('Jaro-Winkler (hello, helo):', lev.jaro_winkler('hello', 'helo'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffd5ebd5-bdf9-4063-b13a-8cd1f06dad11",
   "metadata": {},
   "source": [
    "Gives higher weight to prefix similarity.\n",
    "Used in name matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd4a44-27da-4297-bb7d-048324403b31",
   "metadata": {},
   "source": [
    "## Hamming Distance\n",
    "Measures differences between equal-length strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f642808-a7aa-4abb-b4b0-941441017eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming (karolin, kathrin): 3\n"
     ]
    }
   ],
   "source": [
    "def hamming(s1, s2):\n",
    "    return sum(el1 != el2 for el1, el2 in zip(s1, s2))\n",
    "\n",
    "print('Hamming (karolin, kathrin):', hamming('karolin', 'kathrin'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bd5ee-affa-4e62-a1c8-19152dea3163",
   "metadata": {},
   "source": [
    "Counts the number of mismatched characters.\n",
    "Used in error detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba2a70-3218-4fc2-9836-2ff09968563d",
   "metadata": {},
   "source": [
    "# 4. Spell Corrector\n",
    "Corrects misspelled words using various techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e52a41a-acc0-4413-b845-c1015967c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
      "   ---------------------------------------- 0.0/7.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.1 MB 640.0 kB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.3/7.1 MB 4.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.6/7.1 MB 5.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.8/7.1 MB 4.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.9/7.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.0/7.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.2/7.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.3/7.1 MB 3.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.5/7.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.5/7.1 MB 3.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.7/7.1 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.9/7.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.1/7.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.2/7.1 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.4/7.1 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/7.1 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.7/7.1 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.8/7.1 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.0/7.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.1/7.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.3/7.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.4/7.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.6/7.1 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.7/7.1 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.8/7.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.0/7.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.1/7.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.3/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.4/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.6/7.1 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.7/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.9/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.0/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.2/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.3/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.4/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.6/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.8/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.9/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.0/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.2/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.3/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.5/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.8/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.9/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.1/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.1/7.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.1/7.1 MB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "284a0883-dd99-4c2a-9aae-3e03ac021e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction for speling: spelling\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Example: Correcting a misspelled word\n",
    "print('Correction for speling:', spell.correction('speling'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b674a9b-e1c8-4445-bcb8-8e56b2df7925",
   "metadata": {},
   "source": [
    "SpellChecker() initializes the spell checking object.\n",
    ".correction() finds the most probable correction for a given word.\n",
    "The sentence correction example fixes common spelling errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404caae3-1b8f-489d-a9fd-99af6ff7146f",
   "metadata": {},
   "source": [
    "# 2: Sentence Correction\n",
    "Corrects an entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd0c363f-41f4-4493-a7bc-1840f0905066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: I lug nap and machine learning\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I luv NLP and machin lerning'\n",
    "corrected = ' '.join([spell.correction(word) or word for word in sentence.split()])\n",
    "print('Corrected Sentence:', corrected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeac725-9c78-4ef7-8fc8-d7f48a30e2c5",
   "metadata": {},
   "source": [
    "# Custom Dictionary\n",
    "Adds domain-specific words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9442ed83-5c97-4cef-9835-bc102d955f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction for nlp: nlp\n"
     ]
    }
   ],
   "source": [
    "spell.word_frequency.load_words(['NLP', 'machine', 'learning'])\n",
    "print('Correction for nlp:', spell.correction('nlp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073518e-1740-4564-bbd1-b53bb982c7f6",
   "metadata": {},
   "source": [
    "# Using TextBlob\n",
    "Uses TextBlob for correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f146511-6e89-45ce-a4b3-a7196fe6cb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob Correction: I had a spelling error\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = TextBlob('I hav a speling eror')\n",
    "print('TextBlob Correction:', text.correct())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443ead5-d221-440c-86e1-a036334d38b3",
   "metadata": {},
   "source": [
    "# 5. Pointwise Mutual Information (PMI)\n",
    "Measures the association between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0ca7902-9c68-46f7-9d86-5d0ed630ba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI: {'NLP': 0.8472978603872037, 'is': -0.25131442828090605, 'amazing': -0.25131442828090605, 'I': -0.25131442828090605, 'love': -0.25131442828090605, 'uses': -0.25131442828090605, 'Python': -0.25131442828090605}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "corpus = ['NLP is amazing', 'I love NLP', 'NLP uses Python']\n",
    "tokens = [word for sentence in corpus for word in sentence.split()]\n",
    "freq = Counter(tokens)\n",
    "\n",
    "pmi = {word: math.log((freq[word] / len(tokens)) / (1 / len(freq))) for word in freq}\n",
    "print(\"PMI:\", pmi)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e27e407e-364a-4aa6-ad67-648e590e3d08",
   "metadata": {},
   "source": [
    "PMI highlights strong word associations.\n",
    "Used in topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f41342-b190-40ee-994d-4dc2db1179a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonicalisation Examples\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Example 1: Lowercasing\n",
    "text = 'Hello World! This is NLP 101.'\n",
    "print('Lowercased:', text.lower())\n",
    "\n",
    "# Example 2: Removing Punctuation\n",
    "text_no_punc = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print('Without Punctuation:', text_no_punc)\n",
    "\n",
    "# Example 3: Expanding Contractions\n",
    "contractions = {\"can't\": \"cannot\", \"won't\": \"will not\", \"I'm\": \"I am\"}\n",
    "text_with_expansion = ' '.join([contractions.get(word, word) for word in text.split()])\n",
    "print('Expanded Contractions:', text_with_expansion)\n",
    "\n",
    "# Example 4: Lemmatization and Stemming\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "words = ['running', 'jumps', 'easily', 'fairly']\n",
    "print('Lemmatization:', [lemmatizer.lemmatize(word) for word in words])\n",
    "print('Stemming:', [stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ad0d1",
   "metadata": {},
   "source": [
    "## 2. Phonetic Hashing\n",
    "### Generating phonetic representations of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phonetic Hashing Examples\n",
    "from phonetics import soundex, metaphone\n",
    "import jellyfish\n",
    "\n",
    "# Example 1: Soundex\n",
    "print('Soundex (Smith):', soundex('Smith'))\n",
    "print('Soundex (Smyth):', soundex('Smyth'))\n",
    "\n",
    "# Example 2: Metaphone\n",
    "print('Metaphone (Smith):', metaphone('Smith'))\n",
    "print('Metaphone (Smyth):', metaphone('Smyth'))\n",
    "\n",
    "# Example 3: Double Metaphone\n",
    "print('Double Metaphone (Smith):', jellyfish.metaphone('Smith'))\n",
    "print('Double Metaphone (Smyth):', jellyfish.metaphone('Smyth'))\n",
    "\n",
    "# Example 4: NYSIIS (New York State Identification and Intelligence System)\n",
    "print('NYSIIS (Smith):', jellyfish.nysiis('Smith'))\n",
    "print('NYSIIS (Smyth):', jellyfish.nysiis('Smyth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b7c0c",
   "metadata": {},
   "source": [
    "## 3. Edit Distance\n",
    "### Measuring similarity between strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit Distance Examples\n",
    "import Levenshtein as lev\n",
    "\n",
    "# Example 1: Levenshtein Distance\n",
    "print('Levenshtein (kitten, sitting):', lev.distance('kitten', 'sitting'))\n",
    "\n",
    "# Example 2: Damerau-Levenshtein Distance\n",
    "print('Damerau-Levenshtein (cat, cast):', lev.damerau_levenshtein('cat', 'cast'))\n",
    "\n",
    "# Example 3: Jaro-Winkler Distance\n",
    "print('Jaro-Winkler (hello, helo):', lev.jaro_winkler('hello', 'helo'))\n",
    "\n",
    "# Example 4: Hamming Distance\n",
    "def hamming(s1, s2):\n",
    "    return sum(el1 != el2 for el1, el2 in zip(s1, s2))\n",
    "print('Hamming (karolin, kathrin):', hamming('karolin', 'kathrin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d69acb",
   "metadata": {},
   "source": [
    "## 4. Spell Corrector\n",
    "### Correcting misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell Corrector Examples\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Example 1: Single word correction\n",
    "print('Correction for speling:', spell.correction('speling'))\n",
    "\n",
    "# Example 2: Sentence correction\n",
    "sentence = 'I luv NLP and machin lerning'\n",
    "corrected_sentence = ' '.join([spell.correction(word) or word for word in sentence.split()])\n",
    "print('Corrected Sentence:', corrected_sentence)\n",
    "\n",
    "# Example 3: Custom dictionary-based correction\n",
    "spell.word_frequency.load_words(['nlp', 'machine', 'learning'])\n",
    "print('Correction for nlp:', spell.correction('nlp'))\n",
    "\n",
    "# Example 4: Using NLP libraries for spell correction (TextBlob)\n",
    "from textblob import TextBlob\n",
    "text = TextBlob('I hav a speling eror')\n",
    "print('TextBlob Correction:', text.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd337a8",
   "metadata": {},
   "source": [
    "## 5. Pointwise Mutual Information (PMI)\n",
    "### Measuring word associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7144fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PMI Examples\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Example 1: PMI for bigrams\n",
    "corpus = ['I love NLP', 'I love Python', 'Python is great']\n",
    "tokens = [word for sentence in corpus for word in sentence.split()]\n",
    "freq = Counter(tokens)\n",
    "total = sum(freq.values())\n",
    "pmi = {word: math.log((freq[word] / total) / (total / len(freq))) for word in freq}\n",
    "print('PMI:', pmi)\n",
    "\n",
    "# Example 2: PMI for word pairs\n",
    "word_pairs = [('NLP', 'Python'), ('Python', 'great'), ('I', 'love')]\n",
    "pmi_pairs = {pair: math.log((freq[pair[0]] * freq[pair[1]]) / (total ** 2)) for pair in word_pairs}\n",
    "print('PMI Pairs:', pmi_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78e2d140-47c4-475b-ab0b-454e63519849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\a1\\jupyter\\lib\\site-packages (from spacy) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\a1\\jupyter\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\a1\\jupyter\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\a1\\jupyter\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\a1\\jupyter\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\a1\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\a1\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\a1\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\a1\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\a1\\jupyter\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\a1\\jupyter\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\a1\\jupyter\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\a1\\jupyter\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/11.8 MB 11.9 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 0.6/11.8 MB 7.2 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/11.8 MB 5.7 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.9/11.8 MB 5.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/11.8 MB 4.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.2/11.8 MB 4.4 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/11.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.5/11.8 MB 4.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/11.8 MB 3.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.8/11.8 MB 3.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/11.8 MB 3.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.9/11.8 MB 3.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.0/11.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/11.8 MB 3.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/11.8 MB 3.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.7/11.8 MB 3.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.8/11.8 MB 3.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.9/11.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.1/11.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.2/11.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/11.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.5/11.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.7/11.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.8/11.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.0/11.8 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.1/11.8 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.3/11.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.4/11.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.6/11.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.7/11.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.9/11.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.0/11.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.2/11.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.3/11.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/11.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.6/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.8/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.9/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.0/11.8 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.3/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.5/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.6/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.8/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.9/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.1/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.2/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.4/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.5/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.6/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.8/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.9/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.1/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.2/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.4/11.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.5/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.8/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.0/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.1/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.3/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.6/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.9/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.0/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.1/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/11.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.4/11.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.6/11.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.7/11.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.9/11.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.2/11.8 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.8 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/11.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/11.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/183.0 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 143.4/183.0 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 183.0/183.0 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.4/122.4 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 235.5/632.6 kB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 532.5/632.6 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 632.6/632.6 kB 5.7 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.1/45.1 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading blis-1.2.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/6.3 MB 3.5 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.4/6.3 MB 4.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.7/6.3 MB 4.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.9/6.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.1/6.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.2/6.3 MB 3.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.4/6.3 MB 3.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.5/6.3 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.7/6.3 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.0/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.1/6.3 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.3/6.3 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.4/6.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.5/6.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.7/6.3 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.8/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.0/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.1/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.3/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.6/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.9/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.0/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.3/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.6/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.9/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.3/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.6/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.9/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.1/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "   ---------------------------------------- 0.0/52.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 52.7/52.7 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/5.4 MB 3.9 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.4/5.4 MB 5.0 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.7/5.4 MB 5.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.8/5.4 MB 4.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.0/5.4 MB 4.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.1/5.4 MB 4.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.3/5.4 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.4/5.4 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.7/5.4 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.9/5.4 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.0/5.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.2/5.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.3/5.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.4/5.4 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.6/5.4 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.7/5.4 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.9/5.4 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.0/5.4 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.2/5.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.5/5.4 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.6/5.4 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.8/5.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.1/5.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.3/5.4 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.6/5.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.8/5.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 4.9/5.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.1/5.4 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.8 kB ? eta -:--:--\n",
      "   -------------------------------------- - 143.4/150.8 kB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.8/150.8 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 shellingham-1.5.4 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bff61f2-1103-4cf1-bee4-30ffd4e97ff4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m nlp\u001b[38;5;241m=\u001b[39mspacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m doc\u001b[38;5;241m=\u001b[39mnlp(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI enjoy learning new things and exploring places. What about you?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "File \u001b[1;32m~\\jupyter\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32m~\\jupyter\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(u\"I enjoy learning new things and exploring places. What about you?\")\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token,token.pos_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e1714-825b-4e5b-9415-8aa783751f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
