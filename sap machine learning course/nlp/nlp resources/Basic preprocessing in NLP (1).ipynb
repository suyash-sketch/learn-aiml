{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8913df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be400e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\akshay\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\akshay\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\akshay\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\akshay\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akshay\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\akshay\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "# Natural Language toolkit\n",
    "# textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361b5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415cc80",
   "metadata": {},
   "source": [
    "# 1.Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b292aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Anti-colonial nationalism is an intellectual framework that preceded, accompanied and followed the process of decolonization in the mid-1900s. Benedict Anderson defined a nation as a socially constructed community that is co-created by individuals who imagine themselves as part of this group.[12][16] He points to the New World as the site that originally conceived of nationalism as a concept, which is defined by its imagination of an ahistorical identity that negates colonialism by definition. This concept of nationalism was exemplified by the transformation of settler colonies into nations, while anti-colonial nationalism is exemplified by movements against colonial powers in the 1900s.\n",
    "\n",
    "Nationalist mobilization in French colonial Africa and British colonial India developed \"when colonial regimes refused to cede rights to their increasingly well-educated colonial subjects\", who formed indigenous elites and strategically adopted and adapted nationalist tactics.[12][217][218] New national identities may cross pre-existing ethnic or linguistic divisions.[12] Anti-colonial independence movements in Africa and Asia in the 1900s were led by individuals who had a set of shared identities and imagined a homeland without external rule. Anderson argues that the racism often experienced as a result of colonial rule and attributed to nationalism is rather due to theories of class.[192]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f524e-0204-41b8-807e-091079938876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58fff1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize # tAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb632c5-631e-48e0-bf97-27fdb2a193de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Akshay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ad529ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akshay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "word_tokens = word_tokenize(text)\n",
    "#word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7093e02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens = sent_tokenize(text) # Shift + Tab\n",
    "#sentence_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5d1ea",
   "metadata": {},
   "source": [
    "# 2.Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd796335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "361b2789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e718c903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'is',\n",
       " '@',\n",
       " '#',\n",
       " '$',\n",
       " '$',\n",
       " 'considered',\n",
       " '%',\n",
       " '^',\n",
       " '&',\n",
       " 'the',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " 'remover',\n",
       " '>',\n",
       " '<',\n",
       " '{',\n",
       " '}',\n",
       " 'of',\n",
       " '|_',\n",
       " '/',\n",
       " '?',\n",
       " '.obs',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " 'tacles']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" He is @#$$considered %^& the ()*+, remover ><{}of  |_ /?.obs<=>tacles \"\n",
    "word_tokens = word_tokenize(text)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9f8baa-527f-46c0-a48c-07815d47bdcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "India = [\"Virat\", \"Rohit\", \"Shami\"]\n",
    "\"Rohit\"    not in  India # Membership operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb9d11ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'is', 'considered', 'the', 'remover', 'of', '|_', '.obstacles']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = [i for i in word_tokens if i not in punctuation]\n",
    "clean_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0590a872-b6e7-412d-9bc3-b02fee4ea498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' He is @#$$considered %^& .', 'the remover ><{}of  |_ /?.obstacles']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" He is @#$$considered %^& . the remover ><{}of  |_ /?.obstacles \"\n",
    "sent_tokens = sent_tokenize(text)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dac140a-36e6-4184-9039-709595be842a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' He is @#$$considered %^& .', 'the remover ><{}of  |_ /?.obstacles']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data1 = [i for i in sent_tokens if i not in punctuation]\n",
    "clean_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec2f6016-477e-494c-97c9-674be43d7bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'is', 'considered', 'the', 'remover', 'of', '|_', '.obstacles']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfd442",
   "metadata": {},
   "source": [
    "# 3.Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "451b9bfc-36b0-4117-93d5-c3761321e25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hiii Lets Go For Dinner Tonight'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"HiIi leTS gO fOR DinNer TonIgHT\" \n",
    "a.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e428d00d-5caf-4425-8fe6-7ba82b847cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hiii lets go for dinner tonight'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fc1f1a3-de27-4a16-b060-3a886c5922ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HIII LETS GO FOR DINNER TONIGHT'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83b0e5e2-a30f-41b9-a205-d0e61354b265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hiii lets go for dinner tonight'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c580e13-014d-4dbf-9e99-a7ace7b6722b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hiii lets go for dinner tonight'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3555169-bd8d-4ba0-8937-6c8824dcadb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "straße\n",
      "strasse\n"
     ]
    }
   ],
   "source": [
    "a = \"StraßE\" \n",
    "print(a.lower()) # alpha,beta,gamma,pie\n",
    "print(a.casefold())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "93580259-41d4-42cd-9767-a7378aa1ae13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "βeta\n",
      "βeta\n"
     ]
    }
   ],
   "source": [
    "a= 'Βeta'\n",
    "print(a.lower())\n",
    "print(a.casefold())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00e761a5-a3cc-4d13-8f85-756ec61fa7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,2,3'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= \"{},{},{}\".format(1,2,3)\n",
    "a # default order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4baf7f5-24cc-4a3d-854d-077ed3d1b1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,1,1'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= \"{1},{0},{0}\".format(1,2,3)\n",
    "a # index order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07c095bf-dbae-4631-b3c1-254c603b0f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,2,3'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= \"{a},{b},{c}\".format(a = 1,b = 2,c =3)\n",
    "a # keyword order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56ac7c-9e9d-42eb-8e90-b272b7f00feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9402a-34b6-47ca-871d-a05ea2dcb3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82a7b5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'Is', 'Considered', 'The', 'Remover', 'Of', '|_', '.obstacles']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlecase = [i.capitalize() for i in clean_data]\n",
    "titlecase\n",
    "# i.upper(),i.title(),i.casefold(),i.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d9e013d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are you?,i am fine'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"Hello HOW arE You?,i am fiNe\"\n",
    "a.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c562d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are you?,i am fine'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e54800c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO HOW ARE YOU?,I AM FINE'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db4f21e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello How Are You?,I Am Fine'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f772068",
   "metadata": {},
   "source": [
    "# 4.stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f440eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f515316-f423-43f3-9da4-2d8bd3d536d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Akshay/nltk_data', 'C:\\\\Users\\\\Akshay\\\\anaconda3\\\\nltk_data', 'C:\\\\Users\\\\Akshay\\\\anaconda3\\\\share\\\\nltk_data', 'C:\\\\Users\\\\Akshay\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Akshay\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa51c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1028)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3211471a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aadi',\n",
       " 'aaj',\n",
       " 'aap',\n",
       " 'aapne',\n",
       " 'aata',\n",
       " 'aati',\n",
       " 'aaya',\n",
       " 'aaye',\n",
       " 'ab',\n",
       " 'abbe',\n",
       " 'abbey',\n",
       " 'abe',\n",
       " 'abhi',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'accha',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'acha',\n",
       " 'achcha',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agar',\n",
       " 'ain',\n",
       " 'aint',\n",
       " \"ain't\",\n",
       " 'aisa',\n",
       " 'aise',\n",
       " 'aisi',\n",
       " 'alag',\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'andar',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'ap',\n",
       " 'apan',\n",
       " 'apart',\n",
       " 'apna',\n",
       " 'apnaa',\n",
       " 'apne',\n",
       " 'apni',\n",
       " 'appear',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'arent',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'arre',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'at',\n",
       " 'aur',\n",
       " 'avum',\n",
       " 'aya',\n",
       " 'aye',\n",
       " 'baad',\n",
       " 'baar',\n",
       " 'bad',\n",
       " 'bahut',\n",
       " 'bana',\n",
       " 'banae',\n",
       " 'banai',\n",
       " 'banao',\n",
       " 'banaya',\n",
       " 'banaye',\n",
       " 'banayi',\n",
       " 'banda',\n",
       " 'bande',\n",
       " 'bandi',\n",
       " 'bane',\n",
       " 'bani',\n",
       " 'bas',\n",
       " 'bata',\n",
       " 'batao',\n",
       " 'bc',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bhai',\n",
       " 'bheetar',\n",
       " 'bhi',\n",
       " 'bhitar',\n",
       " 'bht',\n",
       " 'bilkul',\n",
       " 'bohot',\n",
       " 'bol',\n",
       " 'bola',\n",
       " 'bole',\n",
       " 'boli',\n",
       " 'bolo',\n",
       " 'bolta',\n",
       " 'bolte',\n",
       " 'bolti',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'bro',\n",
       " 'btw',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " \"can't\",\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chahiye',\n",
       " 'chaiye',\n",
       " 'chal',\n",
       " 'chalega',\n",
       " 'chhaiye',\n",
       " 'clearly',\n",
       " \"c'mon\",\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'could',\n",
       " 'couldn',\n",
       " 'couldnt',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'de',\n",
       " 'dede',\n",
       " 'dega',\n",
       " 'degi',\n",
       " 'dekh',\n",
       " 'dekha',\n",
       " 'dekhe',\n",
       " 'dekhi',\n",
       " 'dekho',\n",
       " 'denge',\n",
       " 'dhang',\n",
       " 'di',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " \"didn't\",\n",
       " 'dijiye',\n",
       " 'diya',\n",
       " 'diyaa',\n",
       " 'diye',\n",
       " 'diyo',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'done',\n",
       " 'dono',\n",
       " 'dont',\n",
       " \"don't\",\n",
       " 'doosra',\n",
       " 'doosre',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'dude',\n",
       " 'dunga',\n",
       " 'dungi',\n",
       " 'during',\n",
       " 'dusra',\n",
       " 'dusre',\n",
       " 'dusri',\n",
       " 'dvaara',\n",
       " 'dvara',\n",
       " 'dwaara',\n",
       " 'dwara',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'ek',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'fir',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'gaya',\n",
       " 'gaye',\n",
       " 'gayi',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'ghar',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'haan',\n",
       " 'had',\n",
       " 'hadd',\n",
       " 'hadn',\n",
       " 'hadnt',\n",
       " \"hadn't\",\n",
       " 'hai',\n",
       " 'hain',\n",
       " 'hamara',\n",
       " 'hamare',\n",
       " 'hamari',\n",
       " 'hamne',\n",
       " 'han',\n",
       " 'happens',\n",
       " 'har',\n",
       " 'hardly',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'hasnt',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " 'havent',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " \"here's\",\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hm',\n",
       " 'hmm',\n",
       " 'ho',\n",
       " 'hoga',\n",
       " 'hoge',\n",
       " 'hogi',\n",
       " 'hona',\n",
       " 'honaa',\n",
       " 'hone',\n",
       " 'honge',\n",
       " 'hongi',\n",
       " 'honi',\n",
       " 'hopefully',\n",
       " 'hota',\n",
       " 'hotaa',\n",
       " 'hote',\n",
       " 'hoti',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'hoyenge',\n",
       " 'hoyengi',\n",
       " 'hu',\n",
       " 'hua',\n",
       " 'hue',\n",
       " 'huh',\n",
       " 'hui',\n",
       " 'hum',\n",
       " 'humein',\n",
       " 'humne',\n",
       " 'hun',\n",
       " 'huye',\n",
       " 'huyi',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'idk',\n",
       " 'ie',\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'imo',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'inhe',\n",
       " 'inhi',\n",
       " 'inho',\n",
       " 'inka',\n",
       " 'inkaa',\n",
       " 'inke',\n",
       " 'inki',\n",
       " 'inn',\n",
       " 'inner',\n",
       " 'inse',\n",
       " 'insofar',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " 'ise',\n",
       " 'isi',\n",
       " 'iska',\n",
       " 'iskaa',\n",
       " 'iske',\n",
       " 'iski',\n",
       " 'isme',\n",
       " 'isn',\n",
       " 'isne',\n",
       " 'isnt',\n",
       " \"isn't\",\n",
       " 'iss',\n",
       " 'isse',\n",
       " 'issi',\n",
       " 'isski',\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " 'itna',\n",
       " 'itne',\n",
       " 'itni',\n",
       " 'itno',\n",
       " 'its',\n",
       " \"it's\",\n",
       " 'itself',\n",
       " 'ityaadi',\n",
       " 'ityadi',\n",
       " \"i've\",\n",
       " 'ja',\n",
       " 'jaa',\n",
       " 'jab',\n",
       " 'jabh',\n",
       " 'jaha',\n",
       " 'jahaan',\n",
       " 'jahan',\n",
       " 'jaisa',\n",
       " 'jaise',\n",
       " 'jaisi',\n",
       " 'jata',\n",
       " 'jayega',\n",
       " 'jidhar',\n",
       " 'jin',\n",
       " 'jinhe',\n",
       " 'jinhi',\n",
       " 'jinho',\n",
       " 'jinhone',\n",
       " 'jinka',\n",
       " 'jinke',\n",
       " 'jinki',\n",
       " 'jinn',\n",
       " 'jis',\n",
       " 'jise',\n",
       " 'jiska',\n",
       " 'jiske',\n",
       " 'jiski',\n",
       " 'jisme',\n",
       " 'jiss',\n",
       " 'jisse',\n",
       " 'jitna',\n",
       " 'jitne',\n",
       " 'jitni',\n",
       " 'jo',\n",
       " 'just',\n",
       " 'jyaada',\n",
       " 'jyada',\n",
       " 'k',\n",
       " 'ka',\n",
       " 'kaafi',\n",
       " 'kab',\n",
       " 'kabhi',\n",
       " 'kafi',\n",
       " 'kaha',\n",
       " 'kahaa',\n",
       " 'kahaan',\n",
       " 'kahan',\n",
       " 'kahi',\n",
       " 'kahin',\n",
       " 'kahte',\n",
       " 'kaisa',\n",
       " 'kaise',\n",
       " 'kaisi',\n",
       " 'kal',\n",
       " 'kam',\n",
       " 'kar',\n",
       " 'kara',\n",
       " 'kare',\n",
       " 'karega',\n",
       " 'karegi',\n",
       " 'karen',\n",
       " 'karenge',\n",
       " 'kari',\n",
       " 'karke',\n",
       " 'karna',\n",
       " 'karne',\n",
       " 'karni',\n",
       " 'karo',\n",
       " 'karta',\n",
       " 'karte',\n",
       " 'karti',\n",
       " 'karu',\n",
       " 'karun',\n",
       " 'karunga',\n",
       " 'karungi',\n",
       " 'kaun',\n",
       " 'kaunsa',\n",
       " 'kayi',\n",
       " 'kch',\n",
       " 'ke',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'keh',\n",
       " 'kehte',\n",
       " 'kept',\n",
       " 'khud',\n",
       " 'ki',\n",
       " 'kin',\n",
       " 'kine',\n",
       " 'kinhe',\n",
       " 'kinho',\n",
       " 'kinka',\n",
       " 'kinke',\n",
       " 'kinki',\n",
       " 'kinko',\n",
       " 'kinn',\n",
       " 'kino',\n",
       " 'kis',\n",
       " 'kise',\n",
       " 'kisi',\n",
       " 'kiska',\n",
       " 'kiske',\n",
       " 'kiski',\n",
       " 'kisko',\n",
       " 'kisliye',\n",
       " 'kisne',\n",
       " 'kitna',\n",
       " 'kitne',\n",
       " 'kitni',\n",
       " 'kitno',\n",
       " 'kiya',\n",
       " 'kiye',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'ko',\n",
       " 'koi',\n",
       " 'kon',\n",
       " 'konsa',\n",
       " 'koyi',\n",
       " 'krna',\n",
       " 'krne',\n",
       " 'kuch',\n",
       " 'kuchch',\n",
       " 'kuchh',\n",
       " 'kul',\n",
       " 'kull',\n",
       " 'kya',\n",
       " 'kyaa',\n",
       " 'kyu',\n",
       " 'kyuki',\n",
       " 'kyun',\n",
       " 'kyunki',\n",
       " 'lagta',\n",
       " 'lagte',\n",
       " 'lagti',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'le',\n",
       " 'least',\n",
       " 'lekar',\n",
       " 'lekin',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'li',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'liya',\n",
       " 'liye',\n",
       " 'll',\n",
       " 'lo',\n",
       " 'log',\n",
       " 'logon',\n",
       " 'lol',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'lunga',\n",
       " 'm',\n",
       " 'maan',\n",
       " 'maana',\n",
       " 'maane',\n",
       " 'maani',\n",
       " 'maano',\n",
       " 'magar',\n",
       " 'mai',\n",
       " 'main',\n",
       " 'maine',\n",
       " 'mainly',\n",
       " 'mana',\n",
       " 'mane',\n",
       " 'mani',\n",
       " 'mano',\n",
       " 'many',\n",
       " 'mat',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'mein',\n",
       " 'mera',\n",
       " 'mere',\n",
       " 'merely',\n",
       " 'meri',\n",
       " 'might',\n",
       " 'mightn',\n",
       " 'mightnt',\n",
       " \"mightn't\",\n",
       " 'mil',\n",
       " 'mjhe',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'mujhe',\n",
       " 'must',\n",
       " 'mustn',\n",
       " 'mustnt',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'na',\n",
       " 'naa',\n",
       " 'naah',\n",
       " 'nahi',\n",
       " 'nahin',\n",
       " 'nai',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'ne',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'neeche',\n",
       " 'need',\n",
       " 'needn',\n",
       " 'neednt',\n",
       " \"needn't\",\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nhi',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nope',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'par',\n",
       " 'pata',\n",
       " 'pe',\n",
       " 'pehla',\n",
       " 'pehle',\n",
       " 'pehli',\n",
       " 'people',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'phla',\n",
       " 'phle',\n",
       " 'phli',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'poora',\n",
       " 'poori',\n",
       " 'provides',\n",
       " 'pura',\n",
       " 'puri',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'raha',\n",
       " 'rahaa',\n",
       " 'rahe',\n",
       " 'rahi',\n",
       " 'rakh',\n",
       " 'rakha',\n",
       " 'rakhe',\n",
       " 'rakhen',\n",
       " 'rakhi',\n",
       " 'rakho',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'rehte',\n",
       " 'rha',\n",
       " 'rhaa',\n",
       " 'rhe',\n",
       " 'rhi',\n",
       " 'ri',\n",
       " 'right',\n",
       " 's',\n",
       " 'sa',\n",
       " 'saara',\n",
       " 'saare',\n",
       " 'saath',\n",
       " 'sab',\n",
       " 'sabhi',\n",
       " 'sabse',\n",
       " 'sahi',\n",
       " 'said',\n",
       " 'sakta',\n",
       " 'saktaa',\n",
       " 'sakte',\n",
       " 'sakti',\n",
       " 'same',\n",
       " 'sang',\n",
       " 'sara',\n",
       " 'sath',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'se',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'shan',\n",
       " 'shant',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " 'shouldnt',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'si',\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'soch',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " 'tab',\n",
       " 'tabh',\n",
       " 'tak',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tarah',\n",
       " 'teen',\n",
       " 'teeno',\n",
       " 'teesra',\n",
       " 'teesre',\n",
       " 'teesri',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'tera',\n",
       " 'tere',\n",
       " 'teri',\n",
       " 'th',\n",
       " 'tha',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'thats',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'theek',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " \"there's\",\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'thi',\n",
       " 'thik',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'third',\n",
       " 'this',\n",
       " 'tho',\n",
       " 'thoda',\n",
       " 'thodi',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'tjhe',\n",
       " 'to',\n",
       " 'together',\n",
       " 'toh',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'true',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'tu',\n",
       " 'tujhe',\n",
       " 'tum',\n",
       " 'tumhara',\n",
       " 'tumhare',\n",
       " 'tumhari',\n",
       " 'tune',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'um',\n",
       " 'umm',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unhe',\n",
       " 'unhi',\n",
       " 'unho',\n",
       " 'unhone',\n",
       " 'unka',\n",
       " 'unkaa',\n",
       " 'unke',\n",
       " 'unki',\n",
       " 'unko',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'unn',\n",
       " 'unse',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upar',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'usi',\n",
       " 'using',\n",
       " 'uska',\n",
       " 'uske',\n",
       " 'usne',\n",
       " 'uss',\n",
       " 'usse',\n",
       " 'ussi',\n",
       " 'usually',\n",
       " 'vaala',\n",
       " 'vaale',\n",
       " 'vaali',\n",
       " 'vahaan',\n",
       " 'vahan',\n",
       " 'vahi',\n",
       " 'vahin',\n",
       " 'vaisa',\n",
       " 'vaise',\n",
       " 'vaisi',\n",
       " 'vala',\n",
       " 'vale',\n",
       " 'vali',\n",
       " 'various',\n",
       " 've',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vo',\n",
       " 'waala',\n",
       " 'waale',\n",
       " 'waali',\n",
       " 'wagaira',\n",
       " 'wagairah',\n",
       " 'wagerah',\n",
       " 'waha',\n",
       " 'wahaan',\n",
       " 'wahan',\n",
       " 'wahi',\n",
       " 'wahin',\n",
       " 'waisa',\n",
       " 'waise',\n",
       " 'waisi',\n",
       " 'wala',\n",
       " 'wale',\n",
       " 'wali',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'wasnt',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " 'well',\n",
       " \"we'll\",\n",
       " 'went',\n",
       " 'were',\n",
       " \"we're\",\n",
       " 'weren',\n",
       " 'werent',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'whatever',\n",
       " \"what's\",\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " \"where's\",\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " \"who's\",\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'with',\n",
       " 'within',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words( 'hinglish')[:] #[1200:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28450e34-4214-4716-81fd-e95128273ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "       28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a1 = np.arange(11,45)\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c82a0914-fce9-4367-b914-628eaf0b5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2abb8dee-e6e7-4c04-a2af-c123d195809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count  = stopwords.fileids()\n",
    "total_count\n",
    "#len(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0643078a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')[:10]#\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d32965c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hidden',\n",
       " 'somewhere',\n",
       " 'underneath',\n",
       " 'the',\n",
       " 'glamor',\n",
       " 'of',\n",
       " 'a',\n",
       " 'night',\n",
       " 'perfect',\n",
       " 'home',\n",
       " 'record',\n",
       " 'since',\n",
       " '2012',\n",
       " 'is',\n",
       " 'the',\n",
       " 'inconvenient',\n",
       " 'truth',\n",
       " 'of',\n",
       " 'a',\n",
       " 'faltering',\n",
       " ',',\n",
       " 'steadily',\n",
       " 'depreciating',\n",
       " 'batting',\n",
       " 'unit',\n",
       " '.',\n",
       " 'A',\n",
       " 'batting',\n",
       " 'unit',\n",
       " 'that',\n",
       " 'has',\n",
       " 'been',\n",
       " 'sheltered',\n",
       " 'by',\n",
       " 'a',\n",
       " 'well',\n",
       " 'oiled',\n",
       " ',',\n",
       " 'experienced',\n",
       " 'spin',\n",
       " 'bowling',\n",
       " 'unit',\n",
       " 'and',\n",
       " 'by',\n",
       " 'freaks',\n",
       " 'like',\n",
       " 'Jaiswal',\n",
       " 'and',\n",
       " 'Pant',\n",
       " 'who',\n",
       " '’',\n",
       " 've',\n",
       " 'defied',\n",
       " 'cricket',\n",
       " 'logic',\n",
       " 'to',\n",
       " 'stave',\n",
       " 'them',\n",
       " 'off',\n",
       " 'defeats',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'taken',\n",
       " 'a',\n",
       " 'rather',\n",
       " 'innocuous',\n",
       " 'but',\n",
       " 'extremely',\n",
       " 'effective',\n",
       " 'peck',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Kiwi',\n",
       " 'to',\n",
       " 'rattle',\n",
       " 'this',\n",
       " 'regal',\n",
       " 'Indian',\n",
       " 'elephant',\n",
       " ',',\n",
       " 'and',\n",
       " 'hopefully',\n",
       " 'wake',\n",
       " 'it',\n",
       " 'up',\n",
       " 'from',\n",
       " 'a',\n",
       " 'long',\n",
       " 'hidden',\n",
       " 'batting',\n",
       " 'malaise',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" Hidden somewhere underneath the glamor of a night perfect home record since 2012 is the inconvenient truth of a faltering, steadily depreciating batting unit. A batting unit that has been sheltered by a well oiled, experienced spin bowling unit and by freaks like Jaiswal and Pant who’ve defied cricket logic to stave them off defeats. It has taken a rather innocuous but extremely effective peck by the Kiwi to rattle this regal Indian elephant, and hopefully wake it up from a long hidden batting malaise.\"\"\"\n",
    "text1=word_tokenize(text)\n",
    "text1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfbe4691-3f96-41e0-a003-a7c65ee5393b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'a', 'a', 'a']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_removed = [i for i in text1 if i  in stop]\n",
    "stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9891281",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install indic-nlp-library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.stopwords import stopwords\n",
    "\n",
    "# Marathi stop  words\n",
    "marathi_stopwords = stopwords.get_stopwords('mr')\n",
    "print(\"Marathi Stopwords:\", marathi_stopwords)\n",
    "\n",
    "# Sanskrit stopwords\n",
    "sanskrit_stopwords = stopwords.get_stopwords('sa')\n",
    "print(\"Sanskrit Stopwords:\", sanskrit_stopwords)\n",
    "\n",
    "# Tamil stopwords\n",
    "tamil_stopwords = stopwords.get_stopwords('ta')\n",
    "print(\"Tamil Stopwords:\", tamil_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b24b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cda12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d11e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.hi import STOP_WORDS\n",
    "hindi_stopwords = list(STOP_WORDS)\n",
    "print(hindi_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5c34e",
   "metadata": {},
   "source": [
    "# 5.Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ed9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ddca9",
   "metadata": {},
   "source": [
    "## 5.1 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa3eccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['David', 'Miller', 'ensured', 'South', 'Africa', 'went', 'down', 'swinging', 'by', 'blasting', 'as', 'many', 'as', '48', 'runs', 'in', 'the', 'last', 'three', 'overs', 'and', 'finished', 'unbeaten', 'on', '100', 'off', '67', 'balls']\n",
      "Stemmed Words: ['david', 'mil', 'ens', 'sou', 'afric', 'went', 'down', 'swing', 'by', 'blast', 'as', 'many', 'as', '48', 'run', 'in', 'the', 'last', 'three', 'ov', 'and', 'fin', 'unb', 'on', '100', 'off', '67', 'bal']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "import re\n",
    "\n",
    "# Initialize the Lancaster Stemmer\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "# Given text\n",
    "text = \"David Miller ensured South Africa went down swinging by blasting as many as 48 runs in the last three overs and finished unbeaten on 100 off 67 balls\"\n",
    "# Tokenizing words (removing punctuation and splitting)\n",
    "words = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# Applying Lancaster Stemming\n",
    "stemmed_words = [lancaster.stem(i) for i in words]\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2548874-e7ac-4429-b058-5a8d2a6ff6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  The arguing lawyer presented a strong argument, but the judge argued otherwise.\n",
      "Stemmed Sentence:   the argu lawy pres a strong argu but the judg argu otherw\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "import re\n",
    "\n",
    "# Initialize Lancaster Stemmer\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "# Given sentence\n",
    "sentence = \"The arguing lawyer presented a strong argument, but the judge argued otherwise.\"\n",
    "\n",
    "# Tokenizing words (removing punctuation and splitting)\n",
    "words = re.findall(r'\\b\\w+\\b', sentence.lower())  # Convert to lowercase for consistency\n",
    "\n",
    "# Applying Lancaster Stemming\n",
    "stemmed_words = [lancaster.stem(word) for word in words]\n",
    "\n",
    "# Displaying results\n",
    "print(\"Original Sentence: \", sentence)\n",
    "print(\"Stemmed Sentence:  \", \" \".join(stemmed_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a0890",
   "metadata": {},
   "source": [
    "## 5.2 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0f38b270-a73b-409e-a09d-def77b5aa6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\A1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\A1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  David Miller ensured South Africa went down swinging by blasting as many as 48 runs in the last three overs and finished unbeaten on 100 off 67 balls\n",
      "Lemmatized Sentence: david miller ensure south africa go down swing by blast as many as 48 run in the last three overs and finish unbeaten on 100 off 67 ball\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download WordNet data if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Given sentence\n",
    "sentence = \"David Miller ensured South Africa went down swinging by blasting as many as 48 runs in the last three overs and finished unbeaten on 100 off 67 balls\"\n",
    "# Tokenizing words \n",
    "words = word_tokenize(sentence.lower())  # Convert to lowercase for consistency\n",
    "\n",
    "# Applying Lemmatization (default POS = noun, better results with verb tagging)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
    "\n",
    "# Displaying results\n",
    "print(\"Original Sentence: \", sentence)\n",
    "print(\"Lemmatized Sentence:\", \" \".join(lemmatized_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3222dc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  H e n   e w h e r e   u n e r n e h   h e   g l r   f     n g h   p e r f e c   h e   r e c r   n c e   2 0 1 2     h e   n c n v e n e n   r u h   f     f l e r n g ,   e l   e p r e c n g   b n g   u n .   A   b n g   u n   h   h   b e e n   h e l e r e   b     w e l l   l e ,   e x p e r e n c e   p n   b w l n g   u n   n   b   f r e k   l k e   J w l   n   P n   w h ’ v e   e f e   c r c k e   l g c     v e   h e   f f   e f e .   I   h   k e n     r h e r   n n c u u   b u   e x r e e l   e f f e c v e   p e c k   b   h e   K w     r l e   h   r e g l   I n n   e l e p h n ,   n   h p e f u l l   w k e     u p   f r     l n g   h e n   b n g   l e . "
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "for i in stopwords_removed:\n",
    "    text = lemma.lemmatize(i,pos='v')\n",
    "    print(text, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c881a9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# POS tagging\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m r \u001b[38;5;241m>>\u001b[39m adverb\n\u001b[0;32m      3\u001b[0m n \u001b[38;5;241m>>\u001b[39m noun\n\u001b[0;32m      4\u001b[0m a \u001b[38;5;241m>>\u001b[39m adjective\n",
      "\u001b[1;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "r >> adverb\n",
    "n >> noun\n",
    "a >> adjective\n",
    "v >> verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42420a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chaskarakshay9597@gmail.com'] 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"My Mobile Number is 9967590895\n",
    "        my email id is : chaskarakshay9597@gmail.com\n",
    "        todays date is : 01/10/2022\n",
    "        \"\"\"  \n",
    "result = re.findall('[a-z]{5,15}[0-9]{4}[@a-z.]{4,10}',text)\n",
    "#result = re.findall('[a-z]{1,15}[0-9]{4}[@][a-z]{5}[.][a-z]{3}',text)\n",
    "print(result,len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035f3d7-10dc-49d2-82a6-0643e5c94e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
