{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5942c14",
   "metadata": {},
   "source": [
    "#   <font color = darkblue size =6.5> POS-Tagging  (POS - parts of speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065427ee",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81c6b1",
   "metadata": {},
   "source": [
    "* Process to give tags to the tokens(Har ek word ke samne part of speech ka tag lagega)\n",
    "\n",
    "* used mainly in Named Entity Recognition   eg. Resume parsing\n",
    "\n",
    "* Explain how word used in sentence\n",
    "\n",
    "* sometimes there is a requirement like we have to find all the verb in your text data.\n",
    "  then you can use POS tagging and extract all verbs from data.\n",
    "  \n",
    "* Based on our requirement we can decide which tag to use.\n",
    "\n",
    "Example :\n",
    "\n",
    "Input = 'I left the process'\n",
    "\n",
    "output = [('I','PRON'),('left','VERB'),('the','DET'),('process','NOUN')]\n",
    "\n",
    "\n",
    "\n",
    "Noun : animals,place,person etc. \n",
    "\n",
    "Adjective : Qualities or states of nouns small,big etc\n",
    "\n",
    "Adverb : Describe verb peacefully,carefully etc\n",
    "\n",
    "Pronoun : He,she,they etc \n",
    "\n",
    "Conjugation : joining sentences,phrases,words together : so that,when etc.\n",
    "\n",
    "\n",
    "Tags :\n",
    "\n",
    "\n",
    " JJ : Adjective Tag\n",
    " JJR : adjective comparative 'bigger'\n",
    " JJS : adjective superlative 'biggest'\n",
    " NN : Noun singular 'desk'\n",
    " NNS : Noun plural desks\n",
    " NNP : proper noun singular\n",
    " NNPA : proper noun plural 'Americans'\n",
    " PRP : personal pronoun I,he,she\n",
    " RB : adverb very,silently\n",
    " VB : verb base form\n",
    " VBD verb past tense \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8c1b6",
   "metadata": {},
   "source": [
    "#   <font color = darkblue size =6.5> How POS-Tagging algorithm works ?\n",
    "*  Rule based Pos-Tagging\n",
    "* Stochastic Pos-Tagging \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4545ea",
   "metadata": {},
   "source": [
    "## 1. Rule Based POS-Tagging"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb39eb39",
   "metadata": {},
   "source": [
    "1. First we will give input and create a dictionary of all possible tags of a 1 particular word.\n",
    "\n",
    "2. Then apply the 1000 handwritten rules on input data and finally we will get best suitable tag\n",
    "   of that particular word in the output.\n",
    "   Rules - set of rules to structure sentence."
   ]
  },
  {
   "cell_type": "raw",
   "id": "94cdd343",
   "metadata": {},
   "source": [
    "Input----------->create dictionary--------------> Apply rules\n",
    "                                                  (1000 rules)\n",
    "words              {words:tags}              [word:best_suitable_tag]\n",
    "               get all the possible\n",
    "               tags of a particular\n",
    "               word(tagset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0013e",
   "metadata": {},
   "source": [
    "## 2. Stocastic POS Tagging"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fbfa999",
   "metadata": {},
   "source": [
    "1. If input(word) is given to this technique,then it will check previous tags for that \n",
    "   particular word.\n",
    "    \n",
    "2. Depending on the previous experience, it will give tag of that particular word in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e39ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check previous tags for a particular word\n",
    "check tag sequence \n",
    "\n",
    "example :check some previous experience\n",
    "    the ETLHIVE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b339860",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495bcd9",
   "metadata": {},
   "source": [
    "## Popular ways to convert Text to numbers for NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66950afc",
   "metadata": {},
   "source": [
    "<font color = darkblue size =3>1. CountVectorizer\n",
    "___\n",
    "<font color = darkblue size =3>2. TF-IDF Vectorizer\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77cda9",
   "metadata": {},
   "source": [
    "## 1.Count_Vectorizer (Bag of words)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d4e7805",
   "metadata": {},
   "source": [
    "1. Frequency based - Depending on count/frequency of a word, text will be converted into numerical format.\n",
    "   a) Count_vectorizer\n",
    "   b) TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Prediction based - Deep learning algorithms work at the backend that will predict the vector of a particular text."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e15d077f",
   "metadata": {},
   "source": [
    "1. It will count the number of words in a sentence and give vector form of a particular word.\n",
    "\n",
    "2. Total number of columns = Number of unique words in the corpus or all documents.\n",
    "\n",
    "3. we will create a vocabulary of words. we will decide how many words to be given to the model.\n",
    "   we cannot give all the 1-2 lakh words to the model because it will create dimensionality \n",
    "   problem.\n",
    "\n",
    "4. If there are many features then the model will not work well and its accuracy decreases.Thats\n",
    "   why we have to create a vocabulory of words eg. 10000 words vocabulory.\n",
    "    \n",
    "5. If we are giving any unknown word to our model which is not in our vocabulory, then that word \n",
    "   will be counted in separate column of out of vocabulory(oov)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e28086d9",
   "metadata": {},
   "source": [
    "sentence1 = 'I am learning NLP '\n",
    "sentence2 = 'Python and Machine learning are important for learning NLP abc xyz'\n",
    "sentence3 = 'Data Science is combination of AI and Machine learning nnn papa '\n",
    "\n",
    "           I   am   learning   NLP   Python   and   are   Machine   important for Data   Science is combination  AI oov\n",
    "\n",
    "    \n",
    "sentence1  1    1     1        1      0       0       0      0         0        0   0       0     0     0         0  0\n",
    "\n",
    "sentence2  0    0     2        1       1       1      1      1         1        1    0       0     0     0        0  2\n",
    "\n",
    "sentence3  0    0     1        0       0        1     0      1          0       0    1       1     1     1        1  2\n",
    "\n",
    "\n",
    "Creating a vocabulary : 10000 \n",
    "oov : out of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41114db9",
   "metadata": {},
   "source": [
    "### Disadvantage of Count_Vectorizer "
   ]
  },
  {
   "cell_type": "raw",
   "id": "288d10f5",
   "metadata": {},
   "source": [
    "1. Curse of dimensionality : If there are 1000/10000 columns then also there is problem of dimensionality.\n",
    "   Therefore, accuracy of model decreases.\n",
    "\n",
    "2. Does not provide semantic meaning of word : Machine doesnt understand the semantic meaning of words.\n",
    "   Depending on count of words in a document, machine has created vectors of words.But machine doesnt \n",
    "   understand the meaning of words.\n",
    "   eg. Machine learning\n",
    "       When using count vectorizer, machine is at one place and learning is at another place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f2e09",
   "metadata": {},
   "source": [
    "## 2.TF-IDF_Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa3095",
   "metadata": {},
   "source": [
    "### TF - Term Frequency \n",
    "It's function of frequency of term t appearing in document d ;denoted as TF(t,d)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53d288f3",
   "metadata": {},
   "source": [
    "            Frequency of term t appearing in document d\n",
    "TF(t,d)  =  ------------------------------------------      \n",
    "              Total Number of words in document d \n",
    "            \n",
    "            \n",
    "'I am learning NLP and NLP is different domain'  \n",
    "\n",
    "\n",
    "Total number of words in the document = 9\n",
    "\n",
    "Term frequency of I         = 1/9\n",
    "Term frequency of learning  = 1/9\n",
    "Term frequency of NLP       = 2/9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28f38b",
   "metadata": {},
   "source": [
    "### IDF - Inverse Documents Frequency \n",
    "It's Inverse of frequency of Documents containing the desired term out of all the documents and normalized using log"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f64e9336",
   "metadata": {},
   "source": [
    "                        Number of documents d containing term t\n",
    "Document Frequency  =   ---------------------------------------      \n",
    "      (DF)                    Total Number of documents D\n",
    "\n",
    "           \n",
    "We take log in IDF to normalize(scale down) the 1/DF value.\n",
    "          \n",
    "          IDF(t,D) = log(1/Document Frequency)\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "Doc1   I am learning NLP \n",
    "Doc2   I am learning NLP and NLP is different domain     \n",
    "Doc3   NLP is different domain\n",
    "\n",
    "DF of I  = 2/3\n",
    "IDF of I = log(3/2)\n",
    "\n",
    "DF of NLP  = 3/3\n",
    "IDF of NLP = log(3/3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c672b41d",
   "metadata": {},
   "source": [
    "Note - \n",
    "1) In term frequency, we consider single document.\n",
    "2) In document frequency, we consider multiple documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40940ae",
   "metadata": {},
   "source": [
    "## TF-IDF = TF * IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3aa6a",
   "metadata": {},
   "source": [
    "TF(t,d) * IDF(t,D)   >> TF-IDF value will always be in the float\n",
    "\n",
    "Total number of columns(features) = Number of unique words in the corpus or all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc1     The movie was great,simply great                                                                               \n",
    "\n",
    "Doc2     Acting directing screenplay none of them were good\n",
    "\n",
    "Dec3     Movie was fabulous.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "             Acting     directing     fabulous     good      great       stunts     zenith     movie        simply\n",
    "Doc1         0*log(3)   0*log(3)      0*log(3)   0*log(3) (2/6)*log(3)  0*log(0)    0*log(0) (1/6)*log(3) (1/6)*log(3)\n",
    " \n",
    "Doc2      (1/8)*log(3) (1/8)*log(3)   0*log(3) (1/8)*log(3)  0*log(3)   0*log(0)    0*log(0)  0*log(3)     0*log(3)\n",
    "\n",
    "Doc3        0*log(3)     0*log(3)   (1/3)*log(3) 0*log(3)   0*log(3)    0*log(0)    0*log(0)  0*log(3)     0*log(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f837788",
   "metadata": {},
   "source": [
    "# Why to use TFIDF (Main motive behind TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbd522",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. It will reduce the weightage for the repititive words.\n",
    "2. It will increase the weightage for the rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. TF-IDF measures how important a word is within a document relative to a collection of documents(corpus).\n",
    "\n",
    "2. TF - Frequency of a word in a document. Importance α Frequency of word in a document.\n",
    "   IDF - Relative rarity of a word in a collection of documents. Importance α Rarity of words between document.\n",
    "    \n",
    "3. High importance is given to that word which occurs a lot(common) in a given document and rarely in others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db7069",
   "metadata": {},
   "source": [
    "# Difference between Count vectorizer and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9542a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count_vectorizer\n",
    "1. It will count number of times a word occurs in a document.\n",
    "\n",
    "2. It gives equal weightage to all words ie. it doesnt identify the more important and less important words.\n",
    "\n",
    "\n",
    "# TF-IDF\n",
    "1. It counts number of times a word appears in entire corpus.\n",
    "\n",
    "2. It gives importance/ weight to words\n",
    "\n",
    "    Rare words - more importance                                                                \n",
    "    Repititive words - less importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6e91b",
   "metadata": {},
   "source": [
    "# <font color = darkblue size =5> Drawbacks of Count Vectorizer and TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebff590",
   "metadata": {},
   "source": [
    "* we have large number of dimensions (as many as the vocabulary size)\n",
    "\n",
    "* The sequence of words in a sentence is not represented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d996896",
   "metadata": {},
   "source": [
    "# <font color = darkblue size =5> Popular Pretrained Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e4e3d",
   "metadata": {},
   "source": [
    "<font color = darkblue size =3>1.word2vec : https://code.google.com/archive/p/word2vec/\n",
    "___\n",
    "<font color = darkblue size =3>2.Glove  : https://nlp.stanford.edu/projects/glove/\n",
    "___\n",
    "<font color = darkblue size =3>3.Fasttext : https://fasttext.cc//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de0937",
   "metadata": {},
   "outputs": [],
   "source": [
    " vec1 = 5x + 4y -5z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628584fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "379fd57e",
   "metadata": {},
   "source": [
    "# <font color = darkblue size =4> What is word Embedding?\n",
    "    \n",
    "*  Word2Vec differs from CountVectorizer and TF-IDF vectorizer because Word2Vec represents each word as a vector as opposed to the two other models where the whole document was represented as a matrix of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b0304",
   "metadata": {},
   "source": [
    "                                   WORDS\n",
    "Attributes             King     Queen       Apple       Man      Woman   horse\n",
    "\n",
    "Gender                 +1       -1           0.003      +1        -1      +1        \n",
    "\n",
    "Taste                  0.003    0.003         +1        0.03      0.003   0.004                   \n",
    "\n",
    "Royal                   0.85    0.83          0.003      0.023    0.025   0.003                  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "king-  Man + Woman = vec\n",
    "\n",
    "india-US + dollar = vec (rps)\n",
    "\n",
    "\n",
    "king = +1 0.003 0.85       -1  0.003   0.83\n",
    "man =  +1 0.03 0.023\n",
    "women = -1 0.003 0.025 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec  two types pretrained \n",
    "                    model with own data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8ea6d",
   "metadata": {},
   "source": [
    "# <font color = darkblue size =4> Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b9d00",
   "metadata": {},
   "source": [
    "* Developed By Tomas Mikolov\n",
    "* Uses either COntinous Bag Of Words(CBow) or Skip-gram approach\n",
    "* word2vec accessed through gensim or spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6a247",
   "metadata": {},
   "source": [
    "# <font color = darkblue size =3> CBow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Models Aims to predict the present target word based on the surrounding word\"\n",
    "           contextual input\n",
    "example : Apples are Round and sweet    window_size = 1\n",
    "           \n",
    "    \n",
    "    input                                                     output \n",
    "                             \n",
    "    apples(1,0,0,0,0) 0.23\n",
    "                               projection layer           are (0,1,0,0,0)\n",
    "    Round(0,0,1,0,0) 0.11\n",
    "    \n",
    "    \n",
    "    \n",
    "    input                                                      output\n",
    "    \n",
    "    are  (0,1,0,0,0)0.29\n",
    "                               projection layer \n",
    "                                                             (0,0,1,0,0)\n",
    "    and  (0,0,0,1,0) 0.15\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Input                       projection                  output    \n",
    "    \n",
    "    Apples                      --------------\n",
    "                                -                            Round                          \n",
    "    are                         -\n",
    "                  \n",
    "    and\n",
    "    \n",
    "    sweet\n",
    "    \n",
    "    circle\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Apples are Round and sweet Apples are Round and sweet Apples are Round and sweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755e720",
   "metadata": {},
   "source": [
    "## Skip gram "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e15f3",
   "metadata": {},
   "source": [
    "  example : Apples are Round and sweet    window_size = 1\n",
    "  \n",
    "  \n",
    "  \n",
    "  Input                  projection layer          output\n",
    "  \n",
    "  are(0,1,0,0,0)                              Apples(1,0,0,0,0) \n",
    "                                              round(0,0,1,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a31532a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6af53",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17cc60a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2757418",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f51d6",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564adc94",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1a368",
   "metadata": {},
   "source": [
    "# <font color = darkblue size =5> Word Similarity and Differences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31f9a7",
   "metadata": {},
   "source": [
    "# <font color = darkblue size =3> * Cosine Distance/Similarity\n",
    "# <font color = darkblue size =3> * between -1 and +1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524a36e",
   "metadata": {},
   "source": [
    "                 A  . B\n",
    "similarity =   --------------     A - king (2 i + 5 j -3k)   B - Queen (3 i + 6j-5k)\n",
    "                |A|  * |B|\n",
    "                \n",
    "- 1   words are not similar at all\n",
    " + 1 words are similar or it have similar vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e056f6",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88d554",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabc558",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174def1",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a3636",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c6985",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4d679",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e002b",
   "metadata": {},
   "source": [
    "# <font color = darkblue size =5> N-Grams "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0204062d",
   "metadata": {},
   "source": [
    "* Continous sequences of words\n",
    "\n",
    "   \n",
    "* Different type of n-gram are useful for different type of application\n",
    "\n",
    "    \n",
    "* One of usecase keyphrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i am learning python = 'i am','am learning','learning python'\n",
    "\n",
    "unigram = i,am,learning\n",
    "bigram =i am ,am learning, learning python\n",
    "trigram = i am learning ,am learning python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350f948",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6752fd",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bec50c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57145399",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb645bcd",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5989b6",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185800a8",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word Indexing \n",
    "                           0        150          500                750\n",
    "                        Acting.... directing....fabulous....good...great...stunts....zenith...movie---simply\n",
    "* The movie was great,               0                              2\n",
    "  simply great            0                                                                   \n",
    "    \n",
    "* Acting directing \n",
    "  screenplay none of      1         1\n",
    "  them were good\n",
    "\n",
    "* Movie was fabulous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "                     0   1   2   3  4\n",
    "                     0   2\n",
    "        \n",
    "        \n",
    "action  : 0  : 0\n",
    "acting   : 1  0      {0:0,1:0:2:0,3:0}\n",
    "\n",
    "boss    : 2\n",
    "build   : 3\n",
    "    \n",
    "movie :  15 :1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc625542",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic Identification : LDA \n",
    "    \n",
    "doc 100 \n",
    "\n",
    "doc1 : 10    \n",
    "doc2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k /no of topics = 3 = 0,1,2 \n",
    "step 1 :0  1     0    2\n",
    "    * The movie was great,        doc1   : topic 0 : 3 ,topic 1 : 2 ,topic 2 : 1\n",
    "                                  The  : topic 0 : 30 , topic 1 : 2\n",
    "    0      1\n",
    "  simply great      \n",
    "       \n",
    "    each word in document will be assigned random topic \n",
    "        1. topc 0  topic 1 topic 2=   p1\n",
    "        2. movie topic 0 20%      = p2\n",
    "        \n",
    "step 2 :\n",
    "    update topic of each word \n",
    "       \n",
    "step 3 find out for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ea16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "The movie was great   doc_num =100\n",
    "k / no of topics = 3 = 0,1,2\n",
    "step 1 : 0   2    1    2 \n",
    "        The movie was great\n",
    "        \n",
    "sent1 = topic 0 : 1   p1\n",
    "        topic 1 : 1\n",
    "        topic 2 : 2\n",
    "            \n",
    "        movie   topic 0 : 30 \n",
    "                topic 1 : 10\n",
    "                topic 2 : 40     p2 \n",
    "                    \n",
    "step 2 :\n",
    "    update topic of each word \n",
    "step 3 :\n",
    "    find out topic for each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac57cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation Topic Identication : coherence evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df3e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0423c75b",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
