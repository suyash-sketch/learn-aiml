{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab3c66a-a43a-40db-8c2a-edb22d0e9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bed91-d93d-48ed-ab71-cb4eacb17f80",
   "metadata": {},
   "source": [
    "# 1. Lowercasing\n",
    "Converting text to lowercase ensures uniformity (e.g., \"Apple\" and \"apple\" are treated as the same word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af28b29-6675-4187-8c93-c3bfa3345776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased Text: natural language processing is fun!\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing is FUN!\"\n",
    "text_lower = text.lower()\n",
    "print(\"Lowercased Text:\", text_lower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f1469-59ef-4ea1-9b3e-7ddc74921577",
   "metadata": {},
   "source": [
    "# 2. Removing Numbers\n",
    "If numbers are not meaningful in your dataset, you can remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eaff6f6-a0c6-4444-87cf-aa1d2e6fc9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without Numbers: I have  apples and  bananas.\n"
     ]
    }
   ],
   "source": [
    "text = \"I have 2 apples and 3 bananas.\"\n",
    "text_no_numbers = re.sub(r'\\d+', '', text)\n",
    "print(\"Text without Numbers:\", text_no_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b19759-0411-414b-95cd-0231d9c0035d",
   "metadata": {},
   "source": [
    "# 3. Removing Punctuation\n",
    "Stripping punctuation marks to focus on words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4342ff2-938e-44d0-b8e2-57a8fd423c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without Punctuation: Hello Hows it going Great isnt it\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello! How's it going? Great, isn't it?\"\n",
    "text_no_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Text without Punctuation:\", text_no_punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cccee2-1ea0-48a6-9092-7adcadabc103",
   "metadata": {},
   "source": [
    "# 4. Tokenization\n",
    "Splitting text into individual words or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fa0b45-ef2d-4150-9b7b-04cc6f16b7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['Tokenization splits text into meaningful chunks.', \"Let's tokenize it!\"]\n",
      "Word Tokens: ['Tokenization', 'splits', 'text', 'into', 'meaningful', 'chunks', '.', 'Let', \"'s\", 'tokenize', 'it', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Tokenization splits text into meaningful chunks. Let's tokenize it!\"\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)\n",
    "print(\"Word Tokens:\", word_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae97c160-fcb2-460b-af99-1f208877fe62",
   "metadata": {},
   "source": [
    "# 5. Removing Stopwords\n",
    "Stopwords are common words (e.g., \"is\", \"the\", \"and\") that don't add significant meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f1b2c4-3207-4bcd-bd50-370d5506fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without Stopwords: ['example', 'sentence', 'demonstrate', 'stopword', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"This is an example sentence to demonstrate stopword removal.\"\n",
    "tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_no_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Tokens without Stopwords:\", tokens_no_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be5452-3417-4e84-9128-d937b820bfad",
   "metadata": {},
   "source": [
    "# 6. Removing Short Words\n",
    "Filter out words with fewer than 3 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e89c15-576e-43b5-bc69-a57b88734099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without Short Words: ['working', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"I\", \"am\", \"working\", \"on\", \"NLP\"]\n",
    "tokens_no_short = [word for word in tokens if len(word) > 2]\n",
    "print(\"Tokens without Short Words:\", tokens_no_short)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61114ba9-e41f-4ac9-b735-64e329d7e1a1",
   "metadata": {},
   "source": [
    "# 7. Expanding Contractions\n",
    "Convert contractions to their full forms for better context understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8e1fe4-2e7d-45c8-8d2f-be7b9d1e5eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\a1\\jupyter\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\a1\\jupyter\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\a1\\jupyter\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\a1\\jupyter\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a65ecf0-5068-4b7a-b9b3-57819b526863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Text: I cannot go because it is raining.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Dictionary for contraction mapping\n",
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    # Add more contractions as needed\n",
    "}\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n",
    "    return contractions_pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
    "\n",
    "# Example text\n",
    "text = \"I can't go because it's raining.\"\n",
    "expanded_text = expand_contractions(text)\n",
    "print(\"Expanded Text:\", expanded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8911e-e4e2-48c9-a005-8a9c5a27a710",
   "metadata": {},
   "source": [
    "# 10. Spelling Correction\n",
    "Fix spelling mistakes in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac6c20ae-4de5-497c-a37e-f9538fcc9707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Text: His sentence has spelling errors.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"This sentnce has speling erors.\"\n",
    "corrected_text = str(TextBlob(text).correct())\n",
    "print(\"Corrected Text:\", corrected_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d6b78-3aaa-4e44-b618-65fd598d1d14",
   "metadata": {},
   "source": [
    "# 11. Removing URLs and Email Addresses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c922aa-14b3-41de-b5ea-b52c8d3e7997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without URLs and Emails: Contact us at example or visit \n"
     ]
    }
   ],
   "source": [
    "text = \"Contact us at example@example.com or visit https://example.com.\"\n",
    "text_no_urls_emails = re.sub(r'http\\S+|www\\S+|@\\S+', '', text)\n",
    "print(\"Text without URLs and Emails:\", text_no_urls_emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3c4e6-bbb3-4136-8d44-c8cb519c8086",
   "metadata": {},
   "source": [
    "# 12. Handling Frequent or Rare Words\n",
    "For rare words, define a frequency threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f5b79c2-7155-4aa2-bfed-59ace213d54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (Frequent Words): ['apple', 'apple', 'apple']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\", \"grape\"]\n",
    "freq = Counter(tokens)\n",
    "threshold = 2\n",
    "filtered_tokens = [word for word in tokens if freq[word] > threshold]\n",
    "print(\"Filtered Tokens (Frequent Words):\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c35ae1-27f4-4ab6-97e5-b3c6538d7e0a",
   "metadata": {},
   "source": [
    "# 13. Removing Non-ASCII Characters\n",
    "Remove characters that are not standard English.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9583a887-fae8-4c7d-9372-703e02c5f6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without Non-ASCII Characters: Ths txt hs nn-SCII chrctrs.\n"
     ]
    }
   ],
   "source": [
    "text = \"Thís tèxt hás nön-ÁSCII cháráctèrs.\"\n",
    "text_ascii = text.encode('ascii', 'ignore').decode()\n",
    "print(\"Text without Non-ASCII Characters:\", text_ascii)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea151f85-f56b-4a3d-860b-9c69310647b5",
   "metadata": {},
   "source": [
    "# 14. Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f29e73-89a3-419c-88f3-a5b17bfe2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without HTML Tags: This is bold and italic text.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_text = \"<p>This is <b>bold</b> and <i>italic</i> text.</p>\"\n",
    "text_no_html = BeautifulSoup(html_text, \"html.parser\").get_text()\n",
    "print(\"Text without HTML Tags:\", text_no_html  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82af76-9c5c-48d4-8724-921fa5ee1957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
